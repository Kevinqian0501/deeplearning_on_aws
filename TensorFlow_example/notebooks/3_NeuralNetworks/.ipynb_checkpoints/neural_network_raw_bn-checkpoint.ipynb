{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Network Example\n",
    "\n",
    "Build a 2-hidden layers fully connected neural network (a.k.a multilayer perceptron) with TensorFlow.\n",
    "\n",
    "- Author: Aymeric Damien\n",
    "- Project: https://github.com/aymericdamien/TensorFlow-Examples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Overview\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/nn1/neural_net2.jpeg\" alt=\"nn\" style=\"width: 400px;\"/>\n",
    "\n",
    "## MNIST Dataset Overview\n",
    "\n",
    "This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flatten and converted to a 1-D numpy array of 784 features (28*28).\n",
    "\n",
    "![MNIST Dataset](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n",
    "\n",
    "More info: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 5000\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "# X = tf.placeholder(\"float\", [None, num_input])\n",
    "# Y = tf.placeholder(\"float\", [None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define layer\n",
    "def batch_norm_wrapper(inputs, is_training, scale, beta, pop_mean, pop_var, decay = 0.999, epsilon = 1e-3):\n",
    "    def calc_moments_in_train():\n",
    "        batch_mean, batch_var = tf.nn.moments(inputs,[0])\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                               pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_var = tf.assign(pop_var,\n",
    "                              pop_var * decay + batch_var * (1 - decay))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "    \n",
    "    def calc_moments_in_predict():\n",
    "        return pop_mean, pop_var\n",
    "    \n",
    "    mean_, var_ = tf.cond(is_training, \n",
    "                          calc_moments_in_train,\n",
    "                          calc_moments_in_predict\n",
    "                         )\n",
    "    \n",
    "    return tf.nn.batch_normalization(inputs,\n",
    "                mean_, var_, beta, scale, epsilon)\n",
    "\n",
    "def fc_batch_norm_relu(inputs, is_training, weight, biase, scale, beta, pop_mean, pop_var):\n",
    "    fc = tf.add(tf.matmul(inputs, weight), biase)\n",
    "    bn = batch_norm_wrapper(fc, is_training = is_training, scale = scale, beta = beta, pop_mean = pop_mean, pop_var = pop_var)\n",
    "    outputs = tf.nn.relu(bn)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def neural_net(x, weights, biases, scales, betas, pop_means, pop_vars, is_training):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = fc_batch_norm_relu(x, is_training, weights['h1'], biases['b1'], \n",
    "                                 scales['scale_1'], betas['bata_1'], pop_means['pop_mean_1'], pop_vars['pop_var_1'])\n",
    "    \n",
    "    layer_2 = fc_batch_norm_relu(layer_1, is_training, weights['h2'], biases['b2'], \n",
    "                                 scales['scale_2'], betas['bata_2'], pop_means['pop_mean_2'], pop_vars['pop_var_2'])\n",
    "    \n",
    "    layer_3 = fc_batch_norm_relu(layer_2, is_training, weights['h3'], biases['b3'], \n",
    "                                 scales['scale_3'], betas['bata_3'], pop_means['pop_mean_3'], pop_vars['pop_var_3'])\n",
    "    \n",
    "    layer_4 = fc_batch_norm_relu(layer_3, is_training, weights['h4'], biases['b4'], \n",
    "                                 scales['scale_4'], betas['bata_4'], pop_means['pop_mean_4'], pop_vars['pop_var_4'])\n",
    "    \n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_4, weights['out']) + biases['out']\n",
    "    \n",
    "    return out_layer\n",
    "\n",
    "#buid_graph\n",
    "def build_graph():\n",
    "    feature_num = 784 # MNIST data input (img shape: 28*28)\n",
    "    n_hidden_1 = 256 \n",
    "    n_hidden_2 = 128\n",
    "    n_hidden_3 = 64\n",
    "    n_hidden_4 = 32\n",
    "    num_class = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "#         'h1': tf.get_variable(name=\"w_h1\",\n",
    "#             shape=[feature_num, n_hidden_1],\n",
    "#             dtype=tf.float32,\n",
    "#             initializer=tf.contrib.layers.xavier_initializer()),\n",
    "        \n",
    "       'h1': tf.Variable(tf.random_normal([feature_num, n_hidden_1], mean=0.0, stddev = 0.1), name=\"w_h1\"),\n",
    "#         'h2': tf.get_variable(name=\"w_h2\",\n",
    "#             shape=[n_hidden_1, n_hidden_2],\n",
    "#             dtype=tf.float32,\n",
    "#             initializer=tf.contrib.layers.xavier_initializer()),\n",
    "       'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], mean=0.0, stddev = 0.1), name=\"w_h2\"),\n",
    "#         'h3': tf.get_variable(name=\"w_h3\",\n",
    "#             shape=[n_hidden_2, n_hidden_3],\n",
    "#             dtype=tf.float32,\n",
    "#             initializer=tf.contrib.layers.xavier_initializer()),\n",
    "       'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3], mean=0.0, stddev = 0.1), name=\"w_h3\"),\n",
    "#         'h4': tf.get_variable(name=\"w_h4\",\n",
    "#             shape=[n_hidden_3, n_hidden_4],\n",
    "#             dtype=tf.float32,\n",
    "#             initializer=tf.contrib.layers.xavier_initializer()),\n",
    "       'h4': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4], mean=0.0, stddev = 0.1), name=\"w_h4\"),\n",
    "#         'out': tf.get_variable(name=\"w_out\",\n",
    "#             shape=[n_hidden_4, num_class],\n",
    "#             dtype=tf.float32,\n",
    "#             initializer=tf.contrib.layers.xavier_initializer()),\n",
    "       'out': tf.Variable(tf.random_normal([n_hidden_4, num_class], mean=0.0, stddev = 0.1), name=\"w_out\")\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1]), name=\"b_1\"),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2]), name=\"b_2\"),\n",
    "        'b3': tf.Variable(tf.random_normal([n_hidden_3]), name=\"b_3\"),\n",
    "        'b4': tf.Variable(tf.random_normal([n_hidden_4]), name=\"b_4\"),\n",
    "        'out': tf.Variable(tf.random_normal([num_class] ), name=\"b_out\")\n",
    "    }\n",
    "\n",
    "    scales = {\n",
    "        'scale_1':tf.Variable(tf.ones([n_hidden_1]), name=\"scale_1\"),\n",
    "        'scale_2':tf.Variable(tf.ones([n_hidden_2]), name=\"scale_2\"),\n",
    "        'scale_3':tf.Variable(tf.ones([n_hidden_3]), name=\"scale_3\"),\n",
    "        'scale_4':tf.Variable(tf.ones([n_hidden_4]), name=\"scale_4\")\n",
    "    }\n",
    "\n",
    "    betas = {\n",
    "        'bata_1':tf.Variable(tf.zeros([n_hidden_1]), name = \"beta_1\"),\n",
    "        'bata_2':tf.Variable(tf.zeros([n_hidden_2]), name = \"beta_2\"),\n",
    "        'bata_3':tf.Variable(tf.zeros([n_hidden_3]), name = \"beta_3\"),\n",
    "        'bata_4':tf.Variable(tf.zeros([n_hidden_4]), name = \"beta_4\")\n",
    "    }\n",
    "\n",
    "    pop_means = {\n",
    "        'pop_mean_1': tf.Variable(tf.zeros([n_hidden_1]),name = \"pop_mean_1\", trainable=False),\n",
    "        'pop_mean_2': tf.Variable(tf.zeros([n_hidden_2]),name = \"pop_mean_2\", trainable=False),\n",
    "        'pop_mean_3': tf.Variable(tf.zeros([n_hidden_3]),name = \"pop_mean_3\", trainable=False),\n",
    "        'pop_mean_4': tf.Variable(tf.zeros([n_hidden_4]),name = \"pop_mean_4\", trainable=False)\n",
    "    }\n",
    "\n",
    "    pop_vars = {\n",
    "        'pop_var_1':tf.Variable(tf.ones([n_hidden_1]),name = \"pop_var_1\", trainable=False),\n",
    "        'pop_var_2':tf.Variable(tf.ones([n_hidden_2]),name = \"pop_var_2\", trainable=False),\n",
    "        'pop_var_3':tf.Variable(tf.ones([n_hidden_3]),name = \"pop_var_3\", trainable=False),\n",
    "        'pop_var_4':tf.Variable(tf.ones([n_hidden_4]),name = \"pop_var_4\", trainable=False)\n",
    "    }\n",
    "\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    with tf.name_scope(\"input\"):\n",
    "        X = tf.placeholder(tf.float32,[None, feature_num],name=\"X\")\n",
    "        Y = tf.placeholder(tf.float32,[None, num_class],name=\"Y\")\n",
    "\n",
    "    with tf.name_scope(\"logits\"):\n",
    "        logits = neural_net(X,weights, biases, scales, betas, pop_means, pop_vars, is_training)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # Define loss and optimizer\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=Y))\n",
    "\n",
    "    with tf.name_scope(\"train_op\"):\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "    # Evaluate model (with test logits, for dropout to be disabled)\n",
    "        correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    return X, Y, logits, loss, train_op, correct_pred, accuracy, learning_rate, is_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "# weights = {\n",
    "#     'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "#     'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "#     'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "# }\n",
    "# biases = {\n",
    "#     'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "#     'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "#     'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create model\n",
    "# def neural_net(x):\n",
    "#     # Hidden fully connected layer with 256 neurons\n",
    "#     layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "#     # Hidden fully connected layer with 256 neurons\n",
    "#     layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "#     # Output fully connected layer with a neuron for each class\n",
    "#     out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "#     return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Construct model\n",
    "# logits = neural_net(X)\n",
    "\n",
    "# # Define loss and optimizer\n",
    "# loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "#     logits=logits, labels=Y))\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "# train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# # Evaluate model (with test logits, for dropout to be disabled)\n",
    "# correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# # Initialize the variables (i.e. assign their default value)\n",
    "# init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss=  2.83873, Training Accuracy= 0.0703125\n",
      "Step 100, Minibatch Loss=  2.01634, Training Accuracy= 0.304688\n",
      "Step 200, Minibatch Loss=  1.98795, Training Accuracy= 0.25\n",
      "Step 300, Minibatch Loss=  1.93425, Training Accuracy= 0.320312\n",
      "Step 400, Minibatch Loss=  1.65757, Training Accuracy= 0.367188\n",
      "Step 500, Minibatch Loss=  1.55434, Training Accuracy= 0.390625\n",
      "Step 600, Minibatch Loss=  1.25784, Training Accuracy= 0.546875\n",
      "Step 700, Minibatch Loss=  0.972251, Training Accuracy= 0.625\n",
      "Step 800, Minibatch Loss=  1.00943, Training Accuracy= 0.65625\n",
      "Step 900, Minibatch Loss=  0.741371, Training Accuracy= 0.757812\n",
      "Step 1000, Minibatch Loss=  0.516295, Training Accuracy= 0.851562\n",
      "Step 1100, Minibatch Loss=  0.502909, Training Accuracy= 0.835938\n",
      "Step 1200, Minibatch Loss=  0.247361, Training Accuracy= 0.953125\n",
      "Step 1300, Minibatch Loss=  0.173175, Training Accuracy= 0.960938\n",
      "Step 1400, Minibatch Loss=  0.101278, Training Accuracy= 0.96875\n",
      "Step 1500, Minibatch Loss=  0.125064, Training Accuracy= 0.976562\n",
      "Step 1600, Minibatch Loss=  0.106453, Training Accuracy= 0.960938\n",
      "Step 1700, Minibatch Loss=  0.0633403, Training Accuracy= 0.984375\n",
      "Step 1800, Minibatch Loss=  0.0515191, Training Accuracy= 0.984375\n",
      "Step 1900, Minibatch Loss=  0.0824552, Training Accuracy= 0.976562\n",
      "Step 2000, Minibatch Loss=  0.0303527, Training Accuracy= 0.992188\n",
      "Step 2100, Minibatch Loss=  0.0666323, Training Accuracy= 0.976562\n",
      "Step 2200, Minibatch Loss=  0.03759, Training Accuracy= 0.992188\n",
      "Step 2300, Minibatch Loss=  0.0374577, Training Accuracy= 0.984375\n",
      "Step 2400, Minibatch Loss=  0.028082, Training Accuracy= 0.984375\n",
      "Step 2500, Minibatch Loss=  0.00598855, Training Accuracy= 1.0\n",
      "Step 2600, Minibatch Loss=  0.0671632, Training Accuracy= 0.984375\n",
      "Step 2700, Minibatch Loss=  0.0334797, Training Accuracy= 0.992188\n",
      "Step 2800, Minibatch Loss=  0.0150473, Training Accuracy= 0.992188\n",
      "Step 2900, Minibatch Loss=  0.0116636, Training Accuracy= 1.0\n",
      "Step 3000, Minibatch Loss=  0.0247174, Training Accuracy= 0.984375\n",
      "Step 3100, Minibatch Loss=  0.0178614, Training Accuracy= 0.992188\n",
      "Step 3200, Minibatch Loss=  0.0302979, Training Accuracy= 0.984375\n",
      "Step 3300, Minibatch Loss=  0.01558, Training Accuracy= 0.992188\n",
      "Step 3400, Minibatch Loss=  0.00404197, Training Accuracy= 1.0\n",
      "Step 3500, Minibatch Loss=  0.0496417, Training Accuracy= 0.976562\n",
      "Step 3600, Minibatch Loss=  0.0276312, Training Accuracy= 0.992188\n",
      "Step 3700, Minibatch Loss=  0.000817619, Training Accuracy= 1.0\n",
      "Step 3800, Minibatch Loss=  0.00362968, Training Accuracy= 1.0\n",
      "Step 3900, Minibatch Loss=  0.0153715, Training Accuracy= 0.992188\n",
      "Step 4000, Minibatch Loss=  0.0386504, Training Accuracy= 0.984375\n",
      "Step 4100, Minibatch Loss=  0.00065178, Training Accuracy= 1.0\n",
      "Step 4200, Minibatch Loss=  0.00402409, Training Accuracy= 1.0\n",
      "Step 4300, Minibatch Loss=  0.0151596, Training Accuracy= 0.992188\n",
      "Step 4400, Minibatch Loss=  0.00250658, Training Accuracy= 1.0\n",
      "Step 4500, Minibatch Loss=  0.0199193, Training Accuracy= 0.992188\n",
      "Step 4600, Minibatch Loss=  0.000752071, Training Accuracy= 1.0\n",
      "Step 4700, Minibatch Loss=  0.000553887, Training Accuracy= 1.0\n",
      "Step 4800, Minibatch Loss=  0.002814, Training Accuracy= 1.0\n",
      "Step 4900, Minibatch Loss=  0.0578951, Training Accuracy= 0.984375\n",
      "Step 5000, Minibatch Loss=  0.0038992, Training Accuracy= 1.0\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9797\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "graph_bn = tf.Graph()\n",
    "with graph_bn.as_default():\n",
    "    with tf.Session(graph = graph_bn) as sess:\n",
    "        X, Y, logits, loss, train_op, correct_pred, accuracy, learning_rate, is_training = build_graph()\n",
    "        # Run the initializer\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for step in range(1, num_steps+1):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(train_op, feed_dict={X: batch_x, Y: batch_y,learning_rate:0.001,is_training:True})\n",
    "            if step % display_step == 0 or step == 1:\n",
    "                # Calculate batch loss and accuracy\n",
    "                loss_, acc = sess.run([loss, accuracy], feed_dict={X: batch_x,\n",
    "                                                                  Y: batch_y,\n",
    "                                                                  is_training:False})\n",
    "                print(\"Step \" + str(step) + \", Minibatch Loss= \" , \\\n",
    "                      \"{}\".format(str(loss_)) + \", Training Accuracy= \" + \\\n",
    "                      \"{}\".format(str(acc)))\n",
    "#                 print (loss)\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        # Calculate accuracy for MNIST test images\n",
    "        print(\"Testing Accuracy:\", \\\n",
    "            sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "                                          Y: mnist.test.labels,\n",
    "                                          is_training:False}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
